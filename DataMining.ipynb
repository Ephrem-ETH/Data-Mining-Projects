{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataMining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "19x29yK-gOCLSh0-rqQTEgr1sKcnS2XLC",
      "authorship_tag": "ABX9TyOnz30fe24FZ2A0YjMxs5VO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ephrem-ETH/Data-Mining-Projects/blob/master/DataMining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jx0VljMbjyG",
        "outputId": "9033ffad-ff6a-43aa-a61c-539d13bd51de"
      },
      "source": [
        "!pip install stop_words\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stop_words\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/cb/d58290804b7a4c5daa42abbbe2a93c477ae53e45541b1825e86f0dfaaf63/stop-words-2018.7.23.tar.gz\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-cp36-none-any.whl size=32919 sha256=e35672ac7b26dc2b74450a3ef83fe532ea8d3e0ef83485a50686e7b616696f97\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/37/6a/2b295e03bd07290f0da95c3adb9a74ba95fbc333aa8b0c7c78\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH4xrl0hmKG5",
        "outputId": "c4ae00f3-6546-4cd3-b1d4-12303c360d12"
      },
      "source": [
        "!pip install apyori\n",
        "!pip install preprocessor\n",
        "!pip install tweet_preprocessor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting apyori\n",
            "  Downloading https://files.pythonhosted.org/packages/5e/62/5ffde5c473ea4b033490617ec5caa80d59804875ad3c3c57c0976533a21a/apyori-1.1.2.tar.gz\n",
            "Building wheels for collected packages: apyori\n",
            "  Building wheel for apyori (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apyori: filename=apyori-1.1.2-cp36-none-any.whl size=5977 sha256=7d3055df32ba51cabd00076dab10e967baa5ecde435be20a050af3725ab74598\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/92/bb/474bbadbc8c0062b9eb168f69982a0443263f8ab1711a8cad0\n",
            "Successfully built apyori\n",
            "Installing collected packages: apyori\n",
            "Successfully installed apyori-1.1.2\n",
            "Collecting preprocessor\n",
            "  Downloading https://files.pythonhosted.org/packages/96/ad/d9f4ffb9bb97d1cb5bcb876b7932571d4dbaa3eff1701ad45d367f0ea27b/preprocessor-1.1.3.tar.gz\n",
            "Building wheels for collected packages: preprocessor\n",
            "  Building wheel for preprocessor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for preprocessor: filename=preprocessor-1.1.3-cp36-none-any.whl size=4478 sha256=cd0edce676256ea78e48c816093cf11be51ff94c5d4422a4f3672feb48343bd1\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/c1/a2/21fbcfd80d76576bbf148991a66f00730f541f265c7600000f\n",
            "Successfully built preprocessor\n",
            "Installing collected packages: preprocessor\n",
            "Successfully installed preprocessor-1.1.3\n",
            "Collecting tweet_preprocessor\n",
            "  Downloading https://files.pythonhosted.org/packages/17/9d/71bd016a9edcef8860c607e531f30bd09b13103c7951ae73dd2bf174163c/tweet_preprocessor-0.6.0-py3-none-any.whl\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDkgOD2pbFnI",
        "outputId": "130fbcde-2004-4cdd-b427-71c06a32f426"
      },
      "source": [
        "# import required libraries\n",
        "import warnings\n",
        "#from mlxtend.frequent_patterns import apriori\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn_pandas import DataFrameMapper\n",
        "from stop_words import get_stop_words\n",
        "from apyori import apriori\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "# from mlxtend.preprocessing import TransactionEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import preprocessor as p\n",
        "import time\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "stop_words = list(get_stop_words('en'))  # About 900 stopwords\n",
        "nltk_words = list(stopwords.words('english'))\n",
        "stop_words.extend(nltk_words)\n",
        "# Global Parameter\n",
        "# stop_words = stopwords.words('english')\n",
        "# I see that some stop words are not removed and added manually in the stop words list\n",
        "stop_words.extend(['what', 'that', 'the', 'are', 'I','have','when', 'If', 'how', 'By', 'Ok', 'aint', 'My', 'not', 'via', 'today', 'TN', 'As', 'In', 'Im', 'from',\n",
        "                   'that', 'this', 'there', 'A', 'it', 'were', 'We', 'our', 'we', 'didnt', 'each', 'us', 'hey', 'one', 'AS', 'you', 'will', 'wouldnt', 'It', 'K', 'Id'])\n",
        "\n",
        "\n",
        "# Load the data sets\n",
        "def load_dataset(filename):\n",
        "    dataset = pd.read_csv(filename)\n",
        "    return dataset\n",
        "\n",
        "# remove unwanted columns\n",
        "\n",
        "\n",
        "def take_wanted_cols(dataset, cols):\n",
        "    dataset = dataset[cols]\n",
        "    return dataset\n",
        "\n",
        "# Preprocess the tweet by preprocessing\n",
        "\n",
        "def preprocess_tweet(row):\n",
        "    text = row['text']\n",
        "    text = p.clean(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocess_tweet_text(tweet):\n",
        "    tweet.lower()\n",
        "    # remove urls\n",
        "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
        "    # Remove user @ references and '#' from tweet\n",
        "    tweet = re.sub(r'\\@\\w+|\\#', '', tweet)\n",
        "    tweet = re.sub(r'\\b\\w{1,3}\\b', '', tweet)\n",
        "    # Remove numbers\n",
        "    tweet = re.sub(r'\\d+', '', tweet)\n",
        "    # remove whitespace\n",
        "    tweet = tweet.strip()\n",
        "    # Remove punctuations\n",
        "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Remove stopwords\n",
        "    tweet_tokens = word_tokenize(tweet)\n",
        "    filtered_words = [w for w in tweet_tokens if not w in stop_words]\n",
        "    ps = PorterStemmer()\n",
        "    stemmed_words = [ps.stem(w) for w in filtered_words]\n",
        "  \n",
        "    return \" \".join(stemmed_words)\n",
        "\n",
        "\n",
        "\n",
        "def frequent_itemsets_engine(tweets):\n",
        "\n",
        "    frequent_itemsets_in_time = apriori(\n",
        "    tweets, min_support=0.01, min_confidance=0.05, min_lift=3, min_length=2, target=\"rules\")\n",
        "\n",
        "    return frequent_itemsets_in_time\n",
        "\n",
        "\n",
        "def frequent_itemsets(data):\n",
        "    date = data['date'].unique()\n",
        "    start_date = date[0]\n",
        "    results = []\n",
        "    for i in range(3, len(date), 3):\n",
        "       \n",
        "        end_date = date[i]\n",
        "        \n",
        "\n",
        "        tweet = data.loc[(data['date'] >=\n",
        "                          start_date) & (data['date'] <= end_date)]['text']\n",
        "        frequent_terms = frequent_itemsets_engine(tweet.tolist())\n",
        "\n",
        "        #x = get_feature_vector(data['text'])\n",
        "        association_rules = list(frequent_terms)\n",
        "        #print(association_Results)\n",
        "\n",
        "\n",
        "        results.append([start_date.strftime(\"%m/%d/%Y\"),\n",
        "                        end_date.strftime(\"%m/%d/%Y\"), association_rules])\n",
        "        start_date = end_date\n",
        "\n",
        "    return results\n",
        "\n",
        "def frequent_topics(data):\n",
        "  dates = data['date'].unique()\n",
        "  frequent_itemsets=[]\n",
        "  frequent_topics_sets = []\n",
        "  for date in dates:\n",
        "    tweet = data.loc[(data['date'] == date)] ['text']\n",
        "    itemsets = frequent_itemsets_engine(tweet.tolist())\n",
        "    frequent_itemsets.append(list(itemsets))\n",
        "  for itemsets in frequent_itemsets:\n",
        "    #rules = itemsets[0]\n",
        "    for item in itemsets:\n",
        "      pair= item[0]\n",
        "      frequent_topics_sets.append(pair)\n",
        "  counter = Counter(frequent_topics_sets)\n",
        "  print(counter)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset('/content/drive/MyDrive/dataset/covid19_tweets.csv')\n",
        "# gain wanted columns from the raw dataset\n",
        "raw_tweet = take_wanted_cols(dataset, ['text', 'date'])\n",
        "\n",
        "# Let's look some records\n",
        "print(raw_tweet.head())\n",
        "print(raw_tweet.info())\n",
        "\n",
        "# drop duplicate records\n",
        "raw_tweet.drop_duplicates(subset='text', inplace=True)\n",
        "# Convert the date field to date format\n",
        "\n",
        "\n",
        "# preprocess data\n",
        "raw_tweet.text = raw_tweet.apply(preprocess_tweet, axis=1)\n",
        "raw_tweet.text = raw_tweet['text'].apply(preprocess_tweet_text)\n",
        "\n",
        "# convert date from string  to datetime fromat\n",
        "raw_tweet['date'] = pd.to_datetime(raw_tweet['date'])\n",
        "\n",
        "\n",
        "print(\"here\")\n",
        "# transform tweet text into vector and combine with the date feature\n",
        "# mapper = DataFrameMapper([\n",
        "#     ('text', TfidfVectorizer()),\n",
        "#     ('date', None),\n",
        "# ])\n",
        "# vectorize = TfidfVectorizer()\n",
        "# x = vectorize.fit_transform(raw_tweet)\n",
        "\n",
        "raw_tweet['date'] = raw_tweet['date'].dt.date\n",
        "# raw_tweet['day'] = raw_tweet['date'].dt.day\n",
        "# raw_tweet['month'] = raw_tweet['date'].dt.month\n",
        "\n",
        "# raw_tweet['twwetsVec'] = list(x.toarray())\n",
        "\n",
        "# print(raw_tweet.head())\n",
        "# features = mapper.fit_transform(raw_tweet)\n",
        "\n",
        "net_tweet = raw_tweet.sort_values('date', ascending=True)\n",
        "\n",
        "date = pd.to_datetime(net_tweet['date'].unique())\n",
        "net_tweet.text = net_tweet['text'].apply(word_tokenize)\n",
        "#dates.sort(key = lambda date: datetime.strptime(date, '%d %b %Y'))\n",
        "print(date)\n",
        "net_tweet['text'].replace('',np.nan,inplace=True)\n",
        "empty = net_tweet['text'].isnull().sum()\n",
        "print(net_tweet.describe())\n",
        "# transactionFiles = net_tweet['text'].tolist()\n",
        "# print(transactionFiles[0:10])\n",
        "        \n",
        "# print(empty)\n",
        "\n",
        "rules = frequent_topics(net_tweet)\n",
        "output = pd.DataFrame.from_dict(rules, orient='index').reset_index()\n",
        "print(output.head(5))\n",
        "print(net_tweet.head(10))\n",
        "#Print the association rules\n",
        "# item_list = []\n",
        "# for term in rules:\n",
        "#   start_time = term[0]\n",
        "#   end_time   = term[1]\n",
        "#   print(\"From \" + start_time + \" \" + \" to \" + end_time )\n",
        "#   print(\" \")\n",
        "  \n",
        "#   association_rule = term[2]\n",
        "#   for i  in range(0,len(association_rule)):\n",
        "#     pair = association_rule[i][0]\n",
        "#     item_list.append(pair)\n",
        "#     print(len(item_list))\n",
        "#     items = [x for x in pair]\n",
        "#     print(\"Rule: \" + items[0] + \" -> \" + items[1])\n",
        "#     print(\"Support: \" + str(association_rule[i][1]))\n",
        "#     print(\"Confidence: \" + str(association_rule[i][2][0][2]))\n",
        "#     print(\"Lift: \" + str(association_rule[i][2][0][3]))\n",
        "#     print(\"Rules:\" + str(association_rule[i][0]))\n",
        "#   print(\"=============================================\")\n",
        "#   print(\" \")\n",
        "\n",
        "# c= Counter(item_list)\n",
        "# print(c)\n",
        "\n",
        "\n",
        "  \n",
        "  # terms = [x for x in pair]\n",
        "  # print(\"Rule:\" + terms[2][0] + \"->\" + terms[1])\n",
        "  # print(\"Support :\" + term[3])\n",
        "  \n",
        "net_tweet.to_csv('dataset.csv')\n",
        "\n",
        "# raw_tweet_ByDate = raw_tweet.groupby('date').text.unique()\n",
        "\n",
        "# print(raw_tweet_ByDate.head(5))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "                                                text                 date\n",
            "0  If I smelled the scent of hand sanitizers toda...  2020-07-25 12:27:21\n",
            "1  Hey @Yankees @YankeesPR and @MLB - wouldn't it...  2020-07-25 12:27:17\n",
            "2  @diane3443 @wdunlap @realDonaldTrump Trump nev...  2020-07-25 12:27:14\n",
            "3  @brookbanktv The one gift #COVID19 has give me...  2020-07-25 12:27:10\n",
            "4  25 July : Media Bulletin on Novel #CoronaVirus...  2020-07-25 12:27:08\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 179108 entries, 0 to 179107\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count   Dtype \n",
            "---  ------  --------------   ----- \n",
            " 0   text    179108 non-null  object\n",
            " 1   date    179108 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 2.7+ MB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFMvnloFivKx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}